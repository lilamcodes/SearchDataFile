##from kevin crosley

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\IBM_ADMIN\\\\Desktop\\\\DevUP\\\\capstone\\\\csv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def dropAgents():\n",
    "    ## This function loads all chat transcripts, a list of agent names,\n",
    "    ## and saves a new file with only chat lines from customers\n",
    "\n",
    "    # import data file from csv in same directory as this file\n",
    "    df = pd.read_csv('df2.csv')\n",
    "    pd.set_option('max_colwidth', 250)\n",
    "\n",
    "    # drop null rows\n",
    "    temp_df = df[df.notnull()]\n",
    "\n",
    "    # read in list of names extracted from August Transcript List column F\n",
    "    dfAgents = pd.read_csv('AgentNames.csv', header=None)\n",
    "    dfAgents.columns = ['names']  # rename column\n",
    "\n",
    "    # create regex pattern of every agent name as an 'or' condition\n",
    "    pattern = '|'.join(dfAgents.names)\n",
    "\n",
    "    # drop rows that contain agent name ('~' flips boolean values, so we can drop the matches)\n",
    "    check = temp_df[~temp_df.Text.str.contains(pattern, regex=True, na=False)]\n",
    "\n",
    "    # save to file\n",
    "    check.to_csv('noAgents.csv', encoding='utf-8', quoting=csv.QUOTE_ALL, index=True)\n",
    "\n",
    "    # calculate drop percentage and print to Terminal\n",
    "    tot = len(temp_df.index)\n",
    "    nam = len(check.index)\n",
    "    nameDrop = nam/tot\n",
    "    print('Filtering agent names dropped ',round(nameDrop * 100, 2), '%, or ', nam, 'rows out of', tot)\n",
    "\n",
    "\n",
    "def loadNoAgents():\n",
    "    ## This function loads in the csv with only chats from customers and outputs\n",
    "    ## a dataframe of this file\n",
    "\n",
    "    # import csv\n",
    "    dfAgents = pd.read_csv('noAgents.csv')\n",
    "\n",
    "    return dfAgents\n",
    "\n",
    "\n",
    "def filterRegex(rx):\n",
    "    ## This function searches non-agent transcripts for a regex passed in to the function\n",
    "\n",
    "    # create empty output df\n",
    "    outDf = pd.DataFrame()\n",
    "\n",
    "    # load data frame using load function\n",
    "    dfAgents = loadNoAgents()\n",
    "\n",
    "    ## Filter results that include the specified key word\n",
    "    # regex here uses (?i), which matches upper and lower case, and \\w*, which matches any suffix\n",
    "    filtered_df = dfAgents[dfAgents.Text.str.contains(rx, regex=True, na=False)]\n",
    "\n",
    "    ## Format into desired csv output\n",
    "    # convert column to its own data frame\n",
    "    df_part = filtered_df.Text.to_frame()\n",
    "\n",
    "    # append it to the empty output df\n",
    "    outDf = outDf.append(df_part)\n",
    "\n",
    "    # output df to csv\n",
    "    outDf.to_csv('filter.csv', encoding='utf-8', quoting=csv.QUOTE_ALL, index=True)\n",
    "    #outDf.rename(columns={'Text': 'utterance'}, inplace=True)\n",
    "\n",
    "    ## Terminal Output\n",
    "    # describe what's being filtered\n",
    "\n",
    "    # calculate percent that was matched\n",
    "    tot = len(dfAgents.index)\n",
    "    filt = len(outDf.index)\n",
    "    matchP = filt/tot\n",
    "\n",
    "    # print to terminal\n",
    "    print('Filtered with --->', rx, '<--- \\n','Results include ',round(matchP * 100, 2), '%, or ', filt, 'rows out of ', tot)\n",
    "\n",
    "    ## Example Other Filters\n",
    "\n",
    "    # Filter results that includes the key words you are looking for\n",
    "    # filtered_df = temp_df[(temp_df.Text.str.contains(\"(?i)low\\w* rate\", regex=True))&\n",
    "    #                       (temp_df.Text.str.contains(\"(?i)interest\", regex=True))]\n",
    "    #\n",
    "    # filtered_df = temp_df[temp_df.Text.str.contains('(?i)refi\\w*', regex=True)]\n",
    "    # You can take the results from filter_df, and further filter it,\n",
    "    #           resulted in another created another data frame\n",
    "    #\n",
    "    # filtered3_df=filtered_df[(temp_df.Text.str.contains(\"(?i)lowest\", regex=True))|\n",
    "    #                          (temp_df.Text.str.contains(\"(?i)best\", regex=True))]\n",
    "    # filtered2_df=filtered_df[(temp_df.Text.str.contains(\"(?i)rush\", regex=True)==False)]\n",
    "    #\n",
    "    #If you wanted to search through responses:\n",
    "    #filtered_df = utterance_df[(utterance_df.Response.str.contains(\"(?i)\", regex=True))]\n",
    "\n",
    "\n",
    "def anonymize(filename):\n",
    "    ## Imports a filtered csv and 'anonymizes' it by removing all time stamps and names\n",
    "    ## a.k.a. removes everything before 3rd colon\n",
    "\n",
    "    # read csv\n",
    "    df = pd.read_csv('intent_csvs/' + filename + '.csv')\n",
    "\n",
    "    # split by colon\n",
    "    df = pd.DataFrame(df.Text.str.split(':', n=3, expand=True))\n",
    "\n",
    "    # drop middle columns\n",
    "    df = df.drop(columns=[0,1,2])\n",
    "    # save as anon version of same file\n",
    "    df.to_csv('./intent_csvs_anon/' + filename + 'Anon.csv', encoding='utf-8', quoting=csv.QUOTE_ALL, index=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "### Script ###\n",
    "\n",
    "# Call dropAgents one time to create noAgents.csv\n",
    "# dropAgents()\n",
    "\n",
    "# set regex search string\n",
    "rx = '(?i) down'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# call filter function\n",
    "# outputs results into csv called 'filter.csv'\n",
    "# filterRegex(rx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: 201701-citibike-tripdata.csv.zip\n",
      "Loading: 201702-citibike-tripdata.csv.zip\n",
      "Loading: 201703-citibike-tripdata.csv.zip\n",
      "Loading: 201704-citibike-tripdata.csv.zip\n",
      "Loading: 201705-citibike-tripdata.csv.zip\n",
      "Loading: 201706-citibike-tripdata.csv.zip\n",
      "Loading: 201707-citibike-tripdata.csv.zip\n",
      "Loading: 201708-citibike-tripdata.csv.zip\n",
      "Loading: 201709-citibike-tripdata.csv.zip\n",
      "Loading: 201710-citibike-tripdata.csv.zip\n",
      "Loading: 201711-citibike-tripdata.csv.zip\n",
      "Loading: 201712-citibike-tripdata.csv.zip\n"
     ]
    }
   ],
   "source": [
    "# set filename for anonymizing\n",
    "filename = 'relocation'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# call anonymizing function\n",
    "anonymize(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output df, use\n",
    "#df.to_csv('output', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check which columns have null\n",
    "#df.isnull().sum()\n",
    "#results show usertype and birthyear has alot of nulls.  \n",
    "#I will keep that in mind and figure out what needs to be done depending on the case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#libraries that contain methods to help preprocess data\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(missing_values = 'NaN', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Double check that birthyear only includes values from 1918-2017 now\n",
    "#dftest=(df.birthyear)\n",
    "#dftest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Checking that there is no birthyears before 1918 now\n",
    "#dftest.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#checking that birthyear as more NaNs now\n",
    "#df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now and output to a new file so I don't have to run this notebook again.\n",
    "df.to_csv('fulldf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
